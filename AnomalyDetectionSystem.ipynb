{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MSc Individual Project: Anomaly Detection for Assisted Independent Living\n",
    "\n",
    "Author: Christina Spanellis\n",
    " \n",
    "Sections 1-4 of this notebook define the necessary method definitions and constants for this project.\n",
    "\n",
    "Section 5 contains code to build and test the system\n",
    "\n",
    "## Sections\n",
    "### 1. [Data preparation and pre-processing](#section1)\n",
    "### 2. [Anomalous Data Generation Module](#section2)\n",
    "### 3. [Prediction Module](#section3)\n",
    "### 4. [Anomaly Detection Module](#section4)\n",
    "### 5. [Running the system](#section5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages for the project\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import pandas\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from keras import models\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from keras.models import Sequential\n",
    "from keras import callbacks\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import seaborn as sns\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "# from keras.metrics import mean_squared_error\n",
    "from numpy import sqrt\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## Section 1: Data preparation and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different data sets had to be cleaned and prepared to be in a suitable format for the prediction module. \n",
    "\n",
    "Data set 1: CASAS HH101\n",
    "\n",
    "Data set 2: CASAS HH102\n",
    "\n",
    "The below definitions define the constants and logic needed for loading and pre-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONSTANTS AND GLOBALS DEFINITION\n",
    "\n",
    "SENSOR_EVENT_WINDOW_SIZE = 19\n",
    "HH101_PREDICTION_SENSORS = [\"M003\", \"LS002\", \"M004\", \"M005\", \"LS005\", \"MA015\", \"M012\", \"M010\"]\n",
    "HH102_PREDICTION_SENSORS = [\"M007\", \"T105\", \"LS008\", \"M004\", \"M002\", \"LS010\", \"MA009\", \"M018\", \"LS002\"]\n",
    "NUMBER_IN_FEATURES = None\n",
    "DATASET = None\n",
    "NUMBER_PREDICTIONS = None\n",
    "ENABLE_PLOTS = 0\n",
    "ANOMALY_SCORE_THRESHOLD = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the hh101 data set\n",
    "def load_hh101():\n",
    "    # load the data set\n",
    "    hh101 = read_csv('datasets/hh101/hh101.csv', names=[\"Sensor\",1,2,\"Value\",\"Type\"])\n",
    "    hh101.drop(columns={1,2,\"Type\"}, inplace=True)\n",
    "    # replace string values and drop unwanted sensor readings\n",
    "    hh101 = hh101[hh101[\"Sensor\"].str.contains(\"BAT\") == False]\n",
    "    hh101[\"Value\"] = hh101[\"Value\"].replace({\"ON\" : 1.0, \"OFF\" : 0.0})\n",
    "    hh101[\"Value\"] = hh101[\"Value\"].replace({\"ABSENT\" : 1.0, \"PRESENT\" : 0.0})\n",
    "    hh101[\"Value\"] = hh101[\"Value\"].replace({\"OPEN\" : 1.0, \"CLOSE\" : 0.0})\n",
    "    # creating a mapping of the sensor names to keep track of them\n",
    "    count = 0\n",
    "    hh101_sensor_id_mapping = {}\n",
    "    for sensor in hh101[\"Sensor\"].values:\n",
    "        if sensor not in hh101_sensor_id_mapping:\n",
    "            hh101_sensor_id_mapping[sensor] = count\n",
    "            count+=1\n",
    "    hh101_reversed_mapping = {y: x for x, y in hh101_sensor_id_mapping.items()}\n",
    "    return (hh101, hh101_sensor_id_mapping, hh101_reversed_mapping)\n",
    "\n",
    "# load the hh102 dataset \n",
    "def load_hh102():\n",
    "    # load the data set\n",
    "    hh102 = read_csv('datasets/hh102/hh102.csv', names=[\"Sensor\",1,2,\"Value\",\"Type\"])\n",
    "    hh102.drop(columns={1,2,\"Type\"}, inplace=True)\n",
    "    # replace string values and drop unwanted sensor readings\n",
    "    hh102 = hh102[hh102[\"Sensor\"].str.contains(\"BAT\") == False]\n",
    "    hh102[\"Value\"] = hh102[\"Value\"].replace({\"ON\" : 1.0, \"OFF\" : 0.0})\n",
    "    hh102[\"Value\"] = hh102[\"Value\"].replace({\"ABSENT\" : 1.0, \"PRESENT\" : 0.0})\n",
    "    hh102[\"Value\"] = hh102[\"Value\"].replace({\"OPEN\" : 1.0, \"CLOSE\" : 0.0})\n",
    "    # creating a mapping of the sensor names to keep track of them\n",
    "    count = 0\n",
    "    hh102_sensor_id_mapping = {}\n",
    "    for sensor in hh102[\"Sensor\"].values:\n",
    "        if sensor not in hh102_sensor_id_mapping:\n",
    "            hh102_sensor_id_mapping[sensor] = count\n",
    "            count+=1\n",
    "    hh102_reversed_mapping = {y: x for x, y in hh102_sensor_id_mapping.items()}\n",
    "    return (hh102, hh102_sensor_id_mapping, hh102_reversed_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_sensors(dataset, sensors, mapping):\n",
    "    for i in range (len(sensors)):\n",
    "        sensors[i] = mapping[sensors[i]]\n",
    "    return dataset.drop(columns=sensors)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def pca_decomp(dataset, reversed_mapping):\n",
    "    dataset = dataset.copy()\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    dataset[dataset.columns] = min_max_scaler.fit_transform(dataset)\n",
    "    pca = PCA(0.9)\n",
    "    components = pca.fit_transform(dataset)\n",
    "    n_pcs = pca.components_.shape[0]\n",
    "    # get the index of the most important feature on EACH component\n",
    "    most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "    initial_feature_names = dataset.columns\n",
    "    # get the most important feature names\n",
    "    most_important_features = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "    most_important_features = [i for n, i in enumerate(most_important_features) if i not in most_important_features[:n]] \n",
    "    exp_var_pca = pca.explained_variance_ratio_\n",
    "    cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "    if (ENABLE_PLOTS):\n",
    "        if (check_stationary(dataset, most_important_features)):\n",
    "            print(\"All selected features stationary\")\n",
    "        check_autocorrelation(dataset, most_important_features, reversed_mapping)\n",
    "\n",
    "    for i in range(len(most_important_features)):\n",
    "            most_important_features[i] = reversed_mapping[most_important_features[i]]\n",
    "    if (ENABLE_PLOTS):\n",
    "        pyplot.figure(figsize=(15, 5))\n",
    "        pyplot.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "        pyplot.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
    "        pyplot.xlabel('Principal Component')\n",
    "        pyplot.ylabel('Explained Variance')\n",
    "        pyplot.xticks(range(0,len(exp_var_pca)))\n",
    "        pyplot.title(\"Cumulative explained variance in \" + DATASET.upper())\n",
    "        pyplot.savefig(\"data_plots/\"+DATASET +\"/pca\")\n",
    "    return most_important_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationary(pca, n_components):\n",
    "    for column in n_components:\n",
    "        result = adfuller(pca[column])[1]\n",
    "        if result > 0.05:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_autocorrelation(dataset, most_important_features, reversed_mapping):\n",
    "    axs = None\n",
    "    if (len(most_important_features)  % 2 == 0):\n",
    "        fig, axs = pyplot.subplots(int(len(most_important_features) / 2),2,  sharex=True, sharey=True, figsize=(11,8))\n",
    "    else:\n",
    "        fig, axs = pyplot.subplots(int(len(most_important_features) / 2)+1,2,  sharex=True, sharey=True, figsize=(11,8))\n",
    "\n",
    "    axis = axs.flat\n",
    "    for i, column in enumerate(most_important_features):\n",
    "        plot_acf(dataset[column], lags=50, ax = axis[i], alpha=0.05, title=\"Sensor \" + reversed_mapping[column]  +\" autocorrelation\")\n",
    "    if (len(axis) > len(most_important_features)):\n",
    "        fig.delaxes(axis[-1])\n",
    "    \n",
    "    fig.supxlabel(\"Lags\")\n",
    "    fig.supylabel(\"ACF\")\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.savefig(\"data_plots/\" + DATASET + \"/autocorrelation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method transforms the data set into a format where the columns represent the various sensor values and segment the data into 20 event sensor windows.\n",
    "\n",
    "This means that each row in the data set represents the activations for the previous 20 sensor event activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(dataset, sensor_id_mapping,create=False):\n",
    "    count = 0\n",
    "    if os.path.isfile(\"datasets/\"  + DATASET +\"/random.csv\"):\n",
    "        selected = pandas.read_csv(\"datasets/\"  + DATASET +\"/selected_data.csv\", index_col=\"Time\")\n",
    "        extracted = pandas.read_csv(\"datasets/\"  + DATASET +\"/extracted_data.csv\", index_col=\"Time\")\n",
    "        columns = [i for i in range (0,len(sensor_id_mapping))]\n",
    "        selected.columns = columns\n",
    "        columns = [i for i in range (0,8)]\n",
    "        extracted.columns = columns\n",
    "        return selected, extracted\n",
    "    else:\n",
    "        data = []\n",
    "        features = []\n",
    "        starting_date_time = datetime.strptime(dataset.index[0], '%Y-%m-%d %H:%M:%S.%f')\n",
    "        starting_date_time = starting_date_time.replace(microsecond=0)\n",
    "        sensor_vals = [0.0] * len(sensor_id_mapping)\n",
    "        feature_vals = [0.0] * 8\n",
    "        sensor_counts = {}\n",
    "        prev_id = 0\n",
    "        prev_dom_sens = 0\n",
    "        event_count = 0 ## counter used to segment the data into sensor event windows\n",
    "        for i, row in dataset.iterrows():\n",
    "            curr_date_time =  datetime.strptime(i, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            curr_date_time = curr_date_time.replace(microsecond=0)\n",
    "            if (event_count >= SENSOR_EVENT_WINDOW_SIZE):\n",
    "                values = [starting_date_time.strftime(\"%m-%d-%Y %H:%M:%S\")]\n",
    "                values.extend(sensor_vals)\n",
    "                data.append(values)\n",
    "                ## new\n",
    "                if (create):\n",
    "                    feature_vals[0] = (curr_date_time - starting_date_time).seconds\n",
    "                    feature_vals[1] = curr_date_time.hour\n",
    "                    feature_vals[2] = max(sensor_counts, key=sensor_counts.get)\n",
    "                    feature_vals[4] = prev_dom_sens\n",
    "                    prev_dom_sens = max(sensor_counts, key=sensor_counts.get)\n",
    "                    sensor_counts[feature_vals[2]] = 0\n",
    "                    feature_vals[3] = curr_date_time.day\n",
    "                    feature_vals[5] = prev_id\n",
    "                    feature_vals[6] = max(sensor_counts, key=sensor_counts.get)\n",
    "                    sensor_counts[feature_vals[6]] = 0\n",
    "                    feature_vals[7] = max(sensor_counts, key=sensor_counts.get)\n",
    "                    values = [starting_date_time.strftime(\"%m-%d-%Y %H:%M:%S\")]\n",
    "                    values.extend(feature_vals)\n",
    "                    features.append(values)\n",
    "                    sensor_counts = {}\n",
    "\n",
    "                ## new\n",
    "                starting_date_time = curr_date_time\n",
    "                for sensor in sensor_id_mapping:\n",
    "                    if \"D\" in sensor or \"M\" in sensor:\n",
    "                        sensor_vals[sensor_id_mapping[sensor]] = 0.0\n",
    "\n",
    "                # sensor_vals = [0.0] * len(sensor_id_mapping)\n",
    "                event_count = 0\n",
    "            event_count +=1\n",
    "            if \"D\" in row[\"Sensor\"] or \"M\" in row[\"Sensor\"]:\n",
    "                if sensor_vals[sensor_id_mapping[row[\"Sensor\"]]] == 0.0:\n",
    "                    sensor_vals[sensor_id_mapping[row[\"Sensor\"]]] = 1.0\n",
    "                else:\n",
    "                    sensor_vals[sensor_id_mapping[row[\"Sensor\"]]] += 1.0\n",
    "            else:\n",
    "                sensor_vals[sensor_id_mapping[row[\"Sensor\"]]] = row[\"Value\"]\n",
    "            if (create):\n",
    "                if sensor_id_mapping[row[\"Sensor\"]] in sensor_counts:\n",
    "                    sensor_counts[sensor_id_mapping[row[\"Sensor\"]]] +=1\n",
    "                else:\n",
    "                    sensor_counts[sensor_id_mapping[row[\"Sensor\"]]] =1\n",
    "            prev_id = sensor_id_mapping[row[\"Sensor\"]]\n",
    "            if row[\"Sensor\"] == \"D001\":\n",
    "                count+=1\n",
    "        columns = [i for i in range (0,len(sensor_id_mapping))]\n",
    "\n",
    "        final_columns = [\"Time\"]\n",
    "        final_columns.extend(columns)\n",
    "        # set the index of the dataframe to be the time column\n",
    "        new_data = pandas.DataFrame.from_records(data, columns=final_columns)\n",
    "        new_data[\"Time\"] = pandas.to_datetime(new_data[\"Time\"], format='%m-%d-%Y %H:%M:%S')\n",
    "        new_data = new_data.set_index(\"Time\")\n",
    "        new_data.to_csv(\"datasets/\"  + DATASET +\"/selected_data.csv\")\n",
    "\n",
    "        ##\n",
    "        extracted_features = None\n",
    "        if (create):\n",
    "            columns = [i for i in range (0,8)]\n",
    "\n",
    "            final_columns = [\"Time\"]\n",
    "            final_columns.extend(columns)\n",
    "            extracted_features =  pandas.DataFrame.from_records(features, columns=final_columns)\n",
    "        ##\n",
    "\n",
    "        extracted_features[\"Time\"] = pandas.to_datetime(extracted_features[\"Time\"], format='%m-%d-%Y %H:%M:%S')\n",
    "        extracted_features = extracted_features.set_index(\"Time\")\n",
    "        extracted_features.to_csv(\"datasets/\"  + DATASET +\"/extracted_data.csv\")\n",
    "        return new_data, extracted_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method plots all the data (non-normalised) after formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cleaned_data(data, reversed_mapping):\n",
    "    changed_legend = data.rename(columns = reversed_mapping)\n",
    "    ax = changed_legend.plot(subplots=True,figsize=(15,30), sharey=True)\n",
    "    ax[int(len(changed_legend.columns) / 2)].set_ylabel(\"Sensor Value\")\n",
    "    pyplot.xticks(rotation=45)\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.savefig(\"data_plots/\" + DATASET + \"/cleaned_data\", dpi=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method transforms the series into a format suitable for a supervised learning problem.\n",
    "\n",
    "Eg. [Sensor1(t-1), Sensor2(t-1), Sensor1(t), Sensor2(t)]\n",
    "\n",
    "Where readings at t-1 represent the sensor activations for the period before the activations at t.\n",
    "\n",
    "Sensor activations at time t then become the ground truth values for activations at t-1 in the prediction module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_series_to_supervised(new_data, prediction_sensors, important_features, reversed_mapping, create=False, extracted=None):\n",
    "    if os.path.isfile(\"datasets/\"  + DATASET +\"/final_selected_dataset.csv\"):\n",
    "        columns = [i for i in range (0,len(reversed_mapping))]\n",
    "        return (pandas.read_csv(\"datasets/\"  + DATASET +\"/final_selected_dataset.csv\", index_col=[0]), pandas.read_csv(\"datasets/\"  + DATASET +\"/normalised_dataset.csv\",  names=columns)),pandas.read_csv(\"datasets/\"  + DATASET +\"/final_extracted_dataset.csv\", index_col=[0])\n",
    "    else:\n",
    "        df = new_data.copy()\n",
    "        # scale values\n",
    "        values = df.values\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        scaled_values = min_max_scaler.fit_transform(values)\n",
    "        normalized_df = pandas.DataFrame(scaled_values)\n",
    "        normalized_df.to_csv(\"datasets/\"  + DATASET +\"/normalised_dataset.csv\")\n",
    "        if (create):\n",
    "            new_values = extracted.values\n",
    "            min_max_scaler = MinMaxScaler()\n",
    "            new_scaled_values = min_max_scaler.fit_transform(new_values)\n",
    "            new_normalized_df = pandas.DataFrame(new_scaled_values)\n",
    "        n_vars = len(normalized_df.columns)\n",
    "        time_Series = normalized_df.copy().set_index(df.index)\n",
    "        cols, names = list(), list()\n",
    "        new_cols = list()\n",
    "        # input sequence (t-n, ... t-1)\n",
    "        for i in range(1, 0, -1):\n",
    "            sequence = normalized_df.shift(i)\n",
    "            sequence = sequence.rename(columns=reversed_mapping)\n",
    "            sequence = sequence[important_features]\n",
    "            cols.append(sequence)\n",
    "            names += [('%s(t-%d)' % (label, i)) for label in sequence.columns]\n",
    "        if (create):\n",
    "             for i in range(1, 0, -1):\n",
    "                sequence = new_normalized_df.shift(i)\n",
    "                new_cols.append(sequence)\n",
    "        # forecast sequence (t, t+1, ... t+n)\n",
    "        for i in range(0, 1):\n",
    "            sequence = normalized_df.shift(-i)\n",
    "            sequence = sequence.rename(columns=reversed_mapping)\n",
    "            sequence = sequence[prediction_sensors]\n",
    "            cols.append(sequence.shift(-i))\n",
    "            names += [('%s(t)' % (label)) for label in sequence.columns]\n",
    "        if (create):\n",
    "            for i in range(0, 1):\n",
    "                sequence = normalized_df.shift(-i)\n",
    "                sequence = sequence.rename(columns=reversed_mapping)\n",
    "                sequence = sequence[prediction_sensors]\n",
    "                new_cols.append(sequence.shift(-i))\n",
    "        # put it all together\n",
    "        agg = concat(cols, axis=1)\n",
    "        agg.columns = names\n",
    "        # drop rows with NaN values\n",
    "        agg.dropna(inplace=True)\n",
    "        agg.to_csv(\"datasets/\"  + DATASET +\"/final_selected_dataset.csv\")\n",
    "\n",
    "        extracted_data = concat(new_cols, axis=1)\n",
    "        extracted_data.dropna(inplace=True)\n",
    "        extracted_data.to_csv(\"datasets/\"  + DATASET +\"/final_extracted_dataset.csv\")\n",
    "\n",
    "        return (agg, normalized_df), extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method plots the activations for the sensors in the data set, grouped into figures by sensor type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normalised_sensor_activations(normalized_df, reversed_mapping):\n",
    "    changed_legend = normalized_df.rename(columns = reversed_mapping)\n",
    "    doors =[]\n",
    "    lights = []\n",
    "    temp = []\n",
    "    motion = []\n",
    "    for key in reversed_mapping:\n",
    "        if \"D\" in reversed_mapping[key]:\n",
    "            doors.append(key)\n",
    "        elif \"L\" in reversed_mapping[key]:\n",
    "            lights.append(key)\n",
    "        elif \"T1\" in reversed_mapping[key]:\n",
    "            temp.append(key)\n",
    "        else:\n",
    "            motion.append(key)\n",
    "    sensors = [doors, lights, temp, motion]\n",
    "    names = [\"Door Sensors\", \"Light Sensors\", \"Temperature Sensors\", \"Motion Sensors\"]\n",
    "    for i, sensor in enumerate(sensors):\n",
    "        if \"Door\" or \"Temperature\" in names[i]:\n",
    "            figsize = (11,4)\n",
    "            if DATASET == \"hh102\":\n",
    "                figsize = (8,20)\n",
    "        if (len(sensor) % 2 == 0):\n",
    "            # fig, axs = pyplot.subplots(int(len(sensor) / 2),2,  sharex=True, sharey=True, figsize=(11,8))\n",
    "            fig, axs = pyplot.subplots(int(len(sensor) / 2),2,  sharex=True, sharey=True, figsize=(11,16))\n",
    "\n",
    "        else:\n",
    "            # fig, axs = pyplot.subplots(int(len(sensor) / 2)+1,2,  sharex=True, sharey=True, figsize=(11,4))\n",
    "            fig, axs = pyplot.subplots(int(len(sensor) / 2)+1,2,  sharex=True, sharey=True, figsize=(11,16))\n",
    "\n",
    "\n",
    "        count = 0\n",
    "        for ax in axs.flat:\n",
    "            if (count < len(sensor)):\n",
    "                ax.plot(changed_legend[reversed_mapping[sensor[count]]])\n",
    "                ax.set_title(reversed_mapping[sensor[count]])\n",
    "                ax.set_xticks([0, 10000, 20000, 30000, 40000, 50000])\n",
    "                ax.set_yticks([0.0, 0.5, 1.0])\n",
    "                count += 1\n",
    "            else:\n",
    "                fig.delaxes(ax)\n",
    "        for ax in axs.flat:\n",
    "            ax.label_outer()\n",
    "        fig.suptitle(names[i])\n",
    "        fig.supxlabel(\"SEW\")\n",
    "        fig.supylabel(\"Sensor Value\")\n",
    "        pyplot.tight_layout()\n",
    "        pyplot.savefig(\"data_plots/\" + DATASET + \"/\" +names[i],dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discover the features with the highest correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlations(dataset, reversed_mapping):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    scaled_values = min_max_scaler.fit_transform(dataset.values)\n",
    "    normalized_df = pandas.DataFrame(scaled_values)\n",
    "    normalized_df = normalized_df.rename(columns = reversed_mapping)\n",
    "    corrmat = normalized_df.corr(method='pearson', min_periods=100)\n",
    "    corrmat = np.abs(corrmat)\n",
    "    sns.set(context=\"paper\", font=\"monospace\")\n",
    "    f, ax = pyplot.subplots(figsize=(12, 9))\n",
    "    sns.heatmap(corrmat, vmax=0.8, square=True, xticklabels = True, yticklabels = True)\n",
    "    pyplot.title(DATASET.upper() + \" Pearson correlation values between sensors (absolute valued).\")\n",
    "    pyplot.xlabel(\"Sensor ID\")\n",
    "    pyplot.ylabel(\"Sensor ID\")\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.savefig(\"data_plots/\" + DATASET + \"/correlations\")\n",
    "    triangluar_corrmat = np.triu(corrmat, k=1)\n",
    "    values = np.where(triangluar_corrmat >= 0.5)\n",
    "    values = list(zip(values[0], values[1]))\n",
    "    for x in range (len(values)):\n",
    "        values[x] = (reversed_mapping[values[x][0]], reversed_mapping[values[x][1]], triangluar_corrmat[values[x][0]][values[x][1]])\n",
    "    return values \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to be separated out into training (70%), testing (20%) and anomalous portions (10%).\n",
    "\n",
    "The anomalous portion of the data is held back for synthetic anomaly injection to later be used to test the AD system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(dataset):\n",
    "    values = dataset.values\n",
    "    train_split = int(0.7 * len(values))\n",
    "    anomaly_split = int(0.9 * len(values))\n",
    "    train = values[:train_split, :]\n",
    "    test = values[train_split:anomaly_split, :]\n",
    "    anomalies = values[anomaly_split:, :]\n",
    "\n",
    "    train_x, train_y = train[:, :NUMBER_IN_FEATURES], train[:, NUMBER_IN_FEATURES:]\n",
    "\n",
    "    test_x, test_y = test[:, :NUMBER_IN_FEATURES], test[:, NUMBER_IN_FEATURES:]\n",
    "\n",
    "    anomaly_x, anomaly_y = anomalies[:, :NUMBER_IN_FEATURES], anomalies[:, NUMBER_IN_FEATURES:]\n",
    "\n",
    "    train_x = np.asarray(train_x).astype(np.float32)\n",
    "    train_y = np.asarray(train_y).astype(np.float32)\n",
    "    test_y = np.asarray(test_y).astype(np.float32)\n",
    "    test_x = np.asarray(test_x).astype(np.float32)\n",
    "    train_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\n",
    "    test_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))\n",
    "    return (train_x, train_y, test_x, test_y, anomaly_x, anomaly_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "\n",
    "## Section 3: Prediction Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section contains the logic to train models for the Prediction Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method to save trained models and plot their loss (used for experimentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_plot(model, history, type):\n",
    "    model.save(\"best_models/\" + DATASET + \"/\" + type + \"/best_model\", history)\n",
    "    if (ENABLE_PLOTS):\n",
    "        pyplot.plot(history.history['loss'], label='Loss')\n",
    "        pyplot.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        pyplot.xlabel(\"Epochs\")\n",
    "        pyplot.ylabel(\"Loss\")\n",
    "        pyplot.title(\"Training and validation loss of final \"+ DATASET.upper() +\" model.\")\n",
    "        pyplot.tight_layout()\n",
    "        pyplot.legend()\n",
    "        pyplot.savefig(\"best_models/\"+ DATASET +\"/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a model suitable for hyper parameter tuning in in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Int('input_lstm_layer', min_value = 50, max_value = 500, step = 50), input_shape=(train_x.shape[1], train_x.shape[2]), return_sequences = True))\n",
    "    model.add(LSTM(hp.Int('final_lstm_layer', min_value = 50, max_value = 500, step = 50)))\n",
    "    model.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    model.add(Dense(NUMBER_PREDICTIONS, activation = hp.Choice('dense_activation', values=['sigmoid'],default='sigmoid')))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics = ['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "    model.add(Dense(NUMBER_PREDICTIONS, activation = 'sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics = ['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and use the keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(x_train, y_train, type):\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        build_model,\n",
    "        objective='mse',\n",
    "        max_trials=20,\n",
    "        directory=\"tensorflow/\"+DATASET+\"/\" + type +\"/\",\n",
    "        project_name=\"models\",\n",
    "        overwrite = False\n",
    "    )\n",
    "    tuner.search(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size = 128,\n",
    "        validation_split=0.2,\n",
    "        epochs = 500,\n",
    "        callbacks=[callbacks.TensorBoard(log_dir=\"/tmp/tb_logs/\"+DATASET, histogram_freq=1), callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30), callbacks.ModelCheckpoint('best_model_es.h5',monitor='val_loss',mode='min',save_best_only=True)],\n",
    "\n",
    "    )\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(model, train_x, train_y, test_x, test_y,test=False):\n",
    "\n",
    "    history = model.fit(train_x, train_y, epochs=80, validation_data=(test_x, test_y))\n",
    "    # save_model_and_plot(model, history)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train new model with best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_model(train_x, train_y, test_x, test_y, extracted=False):\n",
    "    model = None\n",
    "    history = None\n",
    "    type =\"selected\"\n",
    "    if extracted:\n",
    "        type=\"extracted\"\n",
    "    filename = \"best_models/\" + DATASET +\"/\"+ type + \"/best_model\"\n",
    "    if os.path.isdir(filename):\n",
    "        model = models.load_model(filename)\n",
    "    else:\n",
    "        tuner = tune_model(train_x, train_y, type)\n",
    "        model, history = train_final_model(train_x, train_y, test_x, test_y, tuner, type)\n",
    "    return model, history\n",
    "\n",
    "def train_final_model(train_x, train_y, test_x, test_y, tuner, type):\n",
    "    model = tuner.hypermodel.build(tuner.get_best_hyperparameters()[0])\n",
    "    history = model.fit(train_x, train_y, epochs=100, validation_data=(test_x, test_y))\n",
    "    save_model_and_plot(model, history, type)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "def predict(anomaly_x, model, anomaly_y):\n",
    "\n",
    "    predictions = []\n",
    "    errors = []\n",
    "    sensor_errors = []\n",
    "    r2_s = []\n",
    "    for i in range (len(anomaly_x)):\n",
    "        input = anomaly_x[i]\n",
    "        ground_truth = anomaly_y[i]\n",
    "        input = input.reshape((1, 1, input.shape[0]))\n",
    "        pred = model.predict(input, batch_size=1, verbose=0)\n",
    "        pred = pred[0]\n",
    "        predictions.append(pred)\n",
    "        sensor_errors_temp = []\n",
    "        for i in range (len(ground_truth)):\n",
    "            sensor_errors_temp.append(mean_squared_error([ground_truth[i]], [pred[i]]))\n",
    "        sensor_errors.append(sensor_errors_temp)\n",
    "        errors.append(mean_squared_error(ground_truth, pred))\n",
    "        r2_s.append(r2_score(ground_truth, pred))\n",
    "    predictions = np.array(predictions)\n",
    "    pred_sens = HH102_PREDICTION_SENSORS\n",
    "    if DATASET == \"hh101\":\n",
    "        pred_sens = HH101_PREDICTION_SENSORS\n",
    "    df = pandas.DataFrame(data = sensor_errors, columns=pred_sens)        \n",
    "    # return predictions.reshape(predictions.shape[0], predictions.shape[2]), errors\n",
    "    return errors, r2_s, predictions, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensor_mse(selected, extracted):\n",
    "    data = pandas.DataFrame(columns = ['A'])\n",
    "    cols = []\n",
    "    my_pal = {}\n",
    "    pred_sens = HH101_PREDICTION_SENSORS\n",
    "    if DATASET == \"hh102\":\n",
    "        pred_sens = HH102_PREDICTION_SENSORS\n",
    "    for sensor in pred_sens:\n",
    "        data[sensor + \" S\"]= selected[sensor]\n",
    "        data[sensor+ \" E\"]= extracted[sensor]\n",
    "        cols.append(sensor + \" S\")\n",
    "        my_pal[sensor+ \" S\"] = \"g\"\n",
    "        cols.append(sensor+ \" E\")\n",
    "        my_pal[sensor+ \" E\"] = \"b\"\n",
    "    del data[\"A\"]\n",
    "    df = pandas.DataFrame(data, columns=cols)\n",
    "    b = sns.boxplot(data = df, showfliers=False, whis=1.5, palette=my_pal)\n",
    "    # pyplot.xlim(0,24)\n",
    "    b.set_ylabel(\"MSE\", fontsize=12)\n",
    "    b.set_xlabel(\"Sensor\", fontsize=12)\n",
    "    b.set_xticks([0.5, 2.5, 4.5, 6.5, 8.5, 10.5, 12.5, 14.5])\n",
    "\n",
    "    b.set_xticklabels(extracted.columns)\n",
    "    b.set_title(\"MSE per sensor in \" + DATASET.upper() +\" models\", fontsize=14)\n",
    "\n",
    "    hB, = pyplot.plot([0.09,0.09],'g-')\n",
    "    hR, = pyplot.plot([0.09,0.09],'b-')\n",
    "    pyplot.legend((hB, hR),('Selected', 'Extracted'), fontsize=\"x-large\")\n",
    "    hB.set_visible(False)\n",
    "    hR.set_visible(False)\n",
    "    # # b.set_yticks([0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.12, 0.14, 0.16])\n",
    "    sns.set(rc = {'figure.figsize':(10,6)})\n",
    "    sns.set(rc={\"figure.dpi\":700, 'savefig.dpi':700})\n",
    "\n",
    "    f = b.get_figure()\n",
    "    f.savefig(\"data_plots/\" + DATASET+\"/sensor_mse\")\n",
    "    sns.despine(offset = 5, trim = True)\n",
    "\n",
    "def plot_final_mse(selected_hh101, extracted_hh101, selected_hh102, extracted_hh102):\n",
    "    data = pandas.DataFrame(columns = ['A'])\n",
    "    selected_hh101\n",
    "    my_pal = {}\n",
    "    my_pal[\"HH101 Selected\"] = \"g\"\n",
    "    my_pal[\"HH101 Extracted\"] = \"b\"\n",
    "    my_pal[\"HH102 Selected\"] = \"g\"\n",
    "    my_pal[\"HH102 Extracted\"] = \"b\"\n",
    "    data[\"HH101 Selected\"] = selected_hh101\n",
    "    data[\"HH101 Extracted\"] = extracted_hh101\n",
    "    data[\"HH102 Selected\"] = selected_hh102[:len(selected_hh101)]\n",
    "    data[\"HH102 Extracted\"] = extracted_hh102[:len(selected_hh101)]\n",
    "    del data[\"A\"]\n",
    "    df = pandas.DataFrame(data)\n",
    "    b = sns.boxplot(data = df, showfliers=False, whis=1.5, palette=my_pal)\n",
    "    b.set_ylabel(\"MSE\", fontsize=12)\n",
    "    b.set_xlabel(\"Model\", fontsize=12)\n",
    "    b.set_title(\"Final prediction MSE in all models\", fontsize=14)\n",
    "    # # b.set_yticks([0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.12, 0.14, 0.16])\n",
    "    sns.set(rc = {'figure.figsize':(10,6)})\n",
    "    sns.set(rc={\"figure.dpi\":700, 'savefig.dpi':700})\n",
    "\n",
    "    f = b.get_figure()\n",
    "    f.savefig(\"data_plots/final_mse\")\n",
    "    sns.despine(offset = 5, trim = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_training(hh101_selected_history, hh101_extracted_history, hh102_extracted_history, hh102_selected_history):\n",
    "    fig, (ax1, ax2) = pyplot.subplots(1,2, figsize=(10,4), sharey=True)\n",
    "    ax1.plot(hh101_selected_history.history['loss'], label='FS Loss')\n",
    "    ax1.plot(hh101_selected_history.history['val_loss'], label='FS Validation Loss')\n",
    "    ax1.plot(hh101_extracted_history.history['loss'], label='FE loss')\n",
    "    ax1.plot(hh101_extracted_history.history['val_loss'], label='FE Validation Loss')\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "    ax2.plot(hh102_selected_history.history['loss'], label='FS Loss')\n",
    "    ax2.plot(hh102_selected_history.history['val_loss'], label='FS Validation Loss')\n",
    "    ax2.plot(hh102_extracted_history.history['loss'], label='FE loss')\n",
    "    ax2.plot(hh102_extracted_history.history['val_loss'], label='FE Validation Loss')\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "\n",
    "    ax1.set_title('HH101 training and validation loss')\n",
    "    ax2.set_title('HH102 training and validation loss')\n",
    "    pyplot.tight_layout()\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    pyplot.savefig(\"best_models/both_training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Section 2: Anomamlous Data Generation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section contains the logic for the Anomalous Data Generation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method takes the test data and inserts anomalies into a specified fraction of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stats(dataset):\n",
    "    return (dataset.mean(axis=0), dataset.min(axis=0), dataset.max(axis=0))\n",
    "\n",
    "def generate_anomalous_data(stats, anomaly_x, anomaly_y, reversed_mapping):\n",
    "    anomaly_split = 1000\n",
    "    means = stats[0]\n",
    "    mins = stats[1]\n",
    "    maxs = stats[2]\n",
    "    increase_mag = True\n",
    "    number_anomalies = 200\n",
    "    # now need to randomly select one row of data to to alter, so as to not alter the underlying sequence\n",
    "\n",
    "    random_anomaly_x = anomaly_x[:anomaly_split, :]\n",
    "    random_actual_y = anomaly_y[:anomaly_split, :]\n",
    "    random_anomaly_y, random_rows = generate_random_anomaly(anomaly_y[:anomaly_split, :], number_anomalies, maxs, mins, means,reversed_mapping)\n",
    "\n",
    "    activate_anomaly_x = anomaly_x[anomaly_split*2:anomaly_split*3, :]\n",
    "    activate_anomaly_y, activate_rows = generate_intentional_anomaly(anomaly_y[anomaly_split*2:anomaly_split*3, :], number_anomalies, maxs, mins, means,\"activate\", reversed_mapping)\n",
    "   \n",
    "    return (random_anomaly_x, random_anomaly_y, random_rows, random_actual_y),(activate_anomaly_x, activate_anomaly_y, activate_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = np.random\n",
    "def generate_random_anomaly(anomaly_y, number_anomalies, maxs, mins, means, reversed_mapping):\n",
    "    row_ids = random.choice(anomaly_y.shape[0], size=number_anomalies, replace=False)\n",
    "    random_anomalous_y = pandas.DataFrame(anomaly_y)\n",
    "    random_anomalous_y_copy = random_anomalous_y.copy()\n",
    "    for row_id in row_ids:\n",
    "        new_data = random_anomalous_y.loc[row_id]\n",
    "        for y in range(len(new_data)):\n",
    "            if (random.random() < 0.7):\n",
    "                new_data[y] = means[y] + random.uniform(low=random_anomalous_y.loc[row_id][y], high= 1 * (maxs[y] - mins[y]))\n",
    "        random_anomalous_y.loc[row_id] = new_data\n",
    "    plot_anomalsed_data(row_ids, random_anomalous_y, random_anomalous_y_copy, \"random_anomaly\", reversed_mapping)\n",
    "    return random_anomalous_y.values, row_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = np.random\n",
    "def generate_intentional_anomaly(anomaly_y, number_anomalies, maxs, mins, means, type, reversed_mapping):\n",
    "    row_ids = random.choice(anomaly_y.shape[0], size=number_anomalies, replace=False)\n",
    "    intentional_anomalous_y = pandas.DataFrame(anomaly_y)\n",
    "    intentional_anomalous_y_copy = intentional_anomalous_y.copy()\n",
    "    for row_id in row_ids:\n",
    "        new_data = [0.0] * len(intentional_anomalous_y.columns)\n",
    "        for y in range(len(new_data)):\n",
    "            if (intentional_anomalous_y.iloc[row_id][y] == 0.0):\n",
    "                new_data[y] = means[y] + random.uniform(low=mins[y], high= 1 * (maxs[y] - mins[y]))\n",
    "        intentional_anomalous_y.loc[row_id] = new_data\n",
    "    plot_anomalsed_data(row_ids, intentional_anomalous_y, intentional_anomalous_y_copy, \"intentional_anomaly\", reversed_mapping)\n",
    "    return intentional_anomalous_y.values, row_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalsed_data(row_ids, dataset, dataset_copy, type, reversed_mapping):\n",
    "    if (ENABLE_PLOTS):\n",
    "        colors = None\n",
    "        if (DATASET==\"hh101\"):\n",
    "            colors = [\"cornflowerblue\",\"lightsteelblue\",\"mediumblue\",\"blue\",\"slateblue\",\"navy\",\"royalblue\", \"dodgerblue\"]\n",
    "        else:\n",
    "            colors = [\"cornflowerblue\",\"lightsteelblue\",\"mediumblue\",\"blue\",\"slateblue\",\"navy\",\"royalblue\", \"dodgerblue\",\"cyan\"]\n",
    "        fig, (ax1, ax2) = pyplot.subplots(2,1, figsize=(17,12), dpi=300, sharey=True)\n",
    "        ax1.set_xlabel('SEW')\n",
    "        ax1.set_ylabel('Sensor Value')\n",
    "        ax2.set_xlabel('SEW')\n",
    "        ax2.set_ylabel('Sensor Value')\n",
    "        fig.suptitle(\"Normal data vs data with random synthetic anomaly injection\")\n",
    "        dataset_copy.rename(columns=reversed_mapping, inplace=True)\n",
    "        dataset_copy.plot(color=colors, ax=ax1)\n",
    "        dataset.rename(columns=reversed_mapping, inplace=True)\n",
    "        dataset.plot(color=colors, ax=ax2)\n",
    "        for row in row_ids:\n",
    "            ax2.axvspan(row, row, color='red', alpha=0.5)\n",
    "        pyplot.tight_layout()\n",
    "        pyplot.savefig(\"data_plots/\" + DATASET + \"/\" + type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## Section 4: Anomaly Detection Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method estimates the likelihood of an anomaly occuring by comparing the predicted value to the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(model, type, stats, reversed, anomaly_x, anomaly_y):\n",
    "    thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    # thresholds = [0.3]\n",
    "    random_correct = pandas.DataFrame({0.3 : [], 0.4 :[], 0.5: [], 0.6: [], 0.7: []})\n",
    "    random_undetected = pandas.DataFrame({0.3 : [], 0.4 :[], 0.5: [], 0.6: [], 0.7: []})\n",
    "    random_incorrect = pandas.DataFrame({0.3 : [], 0.4 :[], 0.5: [], 0.6: [], 0.7: []})\n",
    "    \n",
    "    intentional_correct = pandas.DataFrame({0.3 : [], 0.4 :[], 0.5: [], 0.6: [], 0.7: []})\n",
    "    intentional_undetected = pandas.DataFrame({0.3 : [], 0.4 :[], 0.5: [], 0.6: [], 0.7: []})\n",
    "    intentional_incorrect = pandas.DataFrame({0.3 : [], 0.4 :[], 0.5: [], 0.6: [], 0.7: []})\n",
    "    \n",
    "    for i in range (10):    \n",
    "        random_anomaly, activate_anomaly = generate_anomalous_data(stats, anomaly_x, anomaly_y, reversed)\n",
    "\n",
    "        actual_random_anomalies = random_anomaly[2]\n",
    "        errors, r2_s, predictions, df = predict(random_anomaly[0], model, random_anomaly[1])\n",
    "        correct_data = {}\n",
    "        incorrect_data = {}\n",
    "        undetected_data = {}\n",
    "        for threshold in thresholds:\n",
    "            random_anomaly_scores, correctly_detected_anomalies, undetected, incorrectly_detected = detect_anomaly(random_anomaly, predictions, \"random\", actual_random_anomalies, threshold)\n",
    "            correct_data[threshold] = correctly_detected_anomalies\n",
    "            undetected_data[threshold] = undetected\n",
    "            incorrect_data[threshold] = incorrectly_detected\n",
    "        random_correct = random_correct.append(correct_data, ignore_index=True)\n",
    "        random_undetected = random_undetected.append(undetected_data, ignore_index=True)\n",
    "        random_incorrect = random_incorrect.append(incorrect_data, ignore_index=True)\n",
    "\n",
    "    \n",
    "        actual_activate_anomalies = activate_anomaly[2]\n",
    "        errors, r2_s, predictions, df = predict(activate_anomaly[0], model, activate_anomaly[1])\n",
    "        correct_data = {}\n",
    "        incorrect_data = {}\n",
    "        undetected_data = {}\n",
    "        for threshold in thresholds:\n",
    "            activate_anomaly_scores, correctly_detected_anomalies, undetected, incorrectly_detected = detect_anomaly(activate_anomaly, predictions, \"intentional\", actual_activate_anomalies, threshold)\n",
    "            correct_data[threshold] = correctly_detected_anomalies\n",
    "            undetected_data[threshold] = undetected\n",
    "            incorrect_data[threshold] = incorrectly_detected\n",
    "        intentional_correct = intentional_correct.append(correct_data, ignore_index=True)\n",
    "        intentional_undetected = intentional_undetected.append(undetected_data, ignore_index=True)\n",
    "        intentional_incorrect = intentional_incorrect.append(incorrect_data, ignore_index=True)\n",
    " \n",
    "    with open(DATASET + '_anomalies.txt', 'a') as f:\n",
    "        for threshold in thresholds:\n",
    "            f.write('Threshold {} with {} \\n'.format(threshold, type))\n",
    "            f.write('\\tRandom Anomalies\\n')\n",
    "            f.write(\"\\t------------------------------\\n\")\n",
    "            f.write('\\tCorrectly detected: mean={:.2f}% std={:.2f}%\\n'.format(random_correct[threshold].mean(), random_correct[threshold].std()))\n",
    "            f.write('\\tUndetected: mean={:.2f}% std={:.2f}%\\n'.format(random_undetected[threshold].mean(), random_undetected[threshold].std()))\n",
    "            f.write('\\tFalsely detected: mean={:.2f}% std={:.2f}%\\n'.format(random_incorrect[threshold].mean(), random_incorrect[threshold].std()))\n",
    "            f.write(\"\\t------------------------------\\n\")\n",
    "            f.write('\\tIntentional Anomalies\\n')\n",
    "            f.write(\"\\t------------------------------\\n\")\n",
    "            f.write('\\tCorrectly detected: mean={:.2f}% std={:.2f}%\\n'.format(intentional_correct[threshold].mean(), intentional_correct[threshold].std()))\n",
    "            f.write('\\tUndetected: mean={:.2f}% std={:.2f}%\\n'.format(intentional_undetected[threshold].mean(), intentional_undetected[threshold].std()))\n",
    "            f.write('\\tFalsely detected: mean={:.2f}% std={:.2f}%\\n'.format(intentional_incorrect[threshold].mean(), intentional_incorrect[threshold].std()))\n",
    "            f.write(\"\\t------------------------------\\n\")\n",
    "\n",
    "\n",
    "     # return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomaly(anomaly_y, predictions, type, actual, threshold):\n",
    "    anomaly_scores = np.empty((predictions.shape[0]))\n",
    "    detected_anomalies = []\n",
    "    correctly_detected_x = []\n",
    "    correctly_detected_y = []\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        anomaly_probability = 0.0\n",
    "        for x, value in enumerate(prediction):\n",
    "            anomaly_probability+=abs(value - anomaly_y[1][i][x])\n",
    "        anomaly_scores[i] = anomaly_probability / len(prediction)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    anomaly_scores = anomaly_scores.reshape(-1, 1)\n",
    "    anomaly_scores = min_max_scaler.fit_transform(anomaly_scores)\n",
    "    undetected_score = []\n",
    "    undetected = []\n",
    "    for i in range(len(anomaly_scores)):\n",
    "        if (anomaly_scores[i] > threshold):\n",
    "            if i in actual:\n",
    "                correctly_detected_x.append(i)\n",
    "                correctly_detected_y.append(anomaly_scores[i])\n",
    "            else:\n",
    "                detected_anomalies.append(i)\n",
    "        else:\n",
    "            if i in actual:\n",
    "                undetected_score.append(anomaly_scores[i])\n",
    "                undetected.append(i)\n",
    "    if (ENABLE_PLOTS):\n",
    "        fig, ax1 = pyplot.subplots(figsize=(10,6))\n",
    "        ax1.plot(anomaly_scores, marker='o', markevery=correctly_detected_x, c='blue', mfc='black',label='anomaly score', mec='black', zorder=1, markersize=8)\n",
    "        ax1.scatter(undetected, undetected_score, marker='X', c='black', zorder=2, sizes= [80] * len(undetected))\n",
    "        ax1.set_xlabel(\"SEW\")\n",
    "        ax1.set_ylabel(\"Anomaly Score\")\n",
    "        fig.suptitle(\"Anomaly scores for \" + type + \" anomaly injection\")\n",
    "        fig.savefig(\"anomaly_plots/\" + DATASET + \"/\" + type)\n",
    "    correctly_detected_anomalies = (len(correctly_detected_y) / len(actual)) * 100\n",
    "    undetected = 100 - correctly_detected_anomalies\n",
    "    incorrectly_detected = len(detected_anomalies)\n",
    "    return anomaly_scores, correctly_detected_anomalies, undetected, incorrectly_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomaly_scores(anomalies, detected, type):\n",
    "    pyplot.plot(anomalies, '-p', markevery=detected, c='blue', mfc='red',label='anomaly score', mec='red', title=\"Anomaly scores for \" + type + \" anomaly injection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## Section 5: Running the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the code to run the system on data sets 1 and 2\n",
    "\n",
    "1. [HH101](#hh101)\n",
    "2. [HH102](#hh102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hh101'></a>\n",
    "\n",
    "### HH101 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET= \"hh101\"\n",
    "ENABLE_PLOTS = 0\n",
    "# Data pre-processing \n",
    "hh101, hh101_sensor_id_mapping, hh101_reversed_mapping = load_hh101()\n",
    "hh101, extracted_features = transform_data(hh101, hh101_sensor_id_mapping, create=True) \n",
    "# Data pre-processing\n",
    "\n",
    "# Feature Selection\n",
    "most_important_features = pca_decomp(hh101, hh101_reversed_mapping)\n",
    "# Feature Selection\n",
    "\n",
    "# Set up data for prediction module\n",
    "(hh101_data, plot_data), extracted_data = transform_series_to_supervised(hh101, HH101_PREDICTION_SENSORS, most_important_features, hh101_reversed_mapping, create=True, extracted=extracted_features)\n",
    "NUMBER_IN_FEATURES = len(most_important_features)\n",
    "NUMBER_PREDICTIONS = len(HH101_PREDICTION_SENSORS)\n",
    "# Set up data for prediction module\n",
    "\n",
    "# Plot some graphs\n",
    "if (ENABLE_PLOTS):\n",
    "    plot_normalised_sensor_activations(plot_data, hh101_reversed_mapping)\n",
    "    plot_cleaned_data(hh101, hh101_reversed_mapping)\n",
    "    values = find_correlations(plot_data, hh101_reversed_mapping)\n",
    "# Plot some graphs\n",
    "\n",
    "# Get final feature selection model\n",
    "train_x, train_y, test_x, test_y, anomaly_x, anomaly_y = create_train_test_split(hh101_data)\n",
    "hh101_selected, hh101_selected_history = get_final_model(train_x, train_y, test_x, test_y)\n",
    "# Get final feature selection model\n",
    "\n",
    "# Inject anomalies\n",
    "random_anomaly, activate_anomaly = generate_anomalous_data(get_stats(np.row_stack((test_y, train_y, anomaly_y))), anomaly_x, anomaly_y, hh101_reversed_mapping)\n",
    "# Inject anomalies\n",
    "\n",
    "# Detect anomalies\n",
    "detect_anomalies(hh101_selected, \"FS\", get_stats(np.row_stack((test_y, train_y, anomaly_y))), hh101_reversed_mapping, anomaly_x, anomaly_y)\n",
    "# Detect anomalies\n",
    "\n",
    "\n",
    "# hh101_selected_error, hh101_selected_r2s, hh101_selected_predictions, hh101_selected_sensor_errors = predict(anomaly_x, hh101_selected, anomaly_y)\n",
    "\n",
    "\n",
    "# Get final feature selection model\n",
    "NUMBER_IN_FEATURES = 8\n",
    "train_x, train_y, test_x, test_y, anomaly_x, anomaly_y = create_train_test_split(extracted_data)\n",
    "hh101_extracted_model, hh101_extracted_history = get_final_model(train_x, train_y, test_x, test_y, True)\n",
    "# Get final feature selection model\n",
    "\n",
    "# Inject anomalies\n",
    "random_anomaly, activate_anomaly = generate_anomalous_data(get_stats(np.row_stack((test_y, train_y, anomaly_y))), anomaly_x, anomaly_y, hh101_reversed_mapping)\n",
    "# Inject anomalies\n",
    "\n",
    "# Detect anomalies\n",
    "detect_anomalies(hh101_extracted_model, \"FE\", get_stats(np.row_stack((test_y, train_y, anomaly_y))), hh101_reversed_mapping, anomaly_x, anomaly_y)\n",
    "# Detect anomalies\n",
    "\n",
    "# hh101_extracted_error, hh101_extracted_r2s, hh101_extracted_predictions, hh101_extracted_sensor_errors = predict(anomaly_x, hh101_extracted_model, anomaly_y)\n",
    "\n",
    "# Plot sensor mse\n",
    "if (ENABLE_PLOTS):\n",
    "    plot_sensor_mse(hh101_selected_sensor_errors, hh101_extracted_sensor_errors)\n",
    "# Plot sensor mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hh102'></a>\n",
    "\n",
    "### HH102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET= \"hh102\"\n",
    "ENABLE_PLOTS = 0\n",
    "\n",
    "# Data pre-processing \n",
    "print(\"Processing dataset...\")\n",
    "hh102, hh102_sensor_id_mapping, hh102_reversed_mapping = load_hh102()\n",
    "hh102, extracted_features = transform_data(hh102, hh102_sensor_id_mapping, create=True) \n",
    "print(\"Dataset processed\")\n",
    "# Data pre-processing\n",
    "\n",
    "# Feature Selection\n",
    "print(\"Selecting features...\")\n",
    "most_important_features = pca_decomp(hh102, hh102_reversed_mapping)\n",
    "# Feature Selection\n",
    "print(\"Features selected\")\n",
    "\n",
    "# Set up data for prediction module\n",
    "print(\"Transforming data to supervised format...\")\n",
    "(hh102_data, plot_data), extracted_data = transform_series_to_supervised(hh102, HH102_PREDICTION_SENSORS, most_important_features, hh102_reversed_mapping, create=True, extracted=extracted_features)\n",
    "\n",
    "NUMBER_IN_FEATURES = len(most_important_features)\n",
    "NUMBER_PREDICTIONS = len(HH102_PREDICTION_SENSORS)\n",
    "\n",
    "# Plot some graphs\n",
    "if (ENABLE_PLOTS):\n",
    "    print(\"Plotting graphs..\")\n",
    "    plot_normalised_sensor_activations(plot_data, hh102_reversed_mapping)\n",
    "    plot_cleaned_data(hh102, hh102_reversed_mapping)\n",
    "    values = find_correlations(plot_data, hh102_reversed_mapping)\n",
    "# Plot some graphs\n",
    "\n",
    "# Get final feature selection model\n",
    "train_x, train_y, test_x, test_y, anomaly_x, anomaly_y = create_train_test_split(hh102_data)\n",
    "hh102_selected, hh102_selected_history = get_final_model(train_x, train_y, test_x, test_y)\n",
    "# Get final feature selection model\n",
    "\n",
    "# Inject anomalies\n",
    "random_anomaly, activate_anomaly = generate_anomalous_data(get_stats(np.row_stack((test_y, train_y, anomaly_y))), anomaly_x, anomaly_y, hh102_reversed_mapping)\n",
    "# Inject anomalies\n",
    "\n",
    "# Detect anomalies\n",
    "detect_anomalies(hh102_selected, \"FS\", get_stats(np.row_stack((test_y, train_y, anomaly_y))), hh102_reversed_mapping, anomaly_x, anomaly_y)\n",
    "# Detect anomalies\n",
    "\n",
    "# hh102_selected_error, hh102_selected_r2s, hh102_selected_predictions, hh102_selected_sensor_errors = predict(anomaly_x, hh102_selected, anomaly_y)\n",
    "\n",
    "# Get final feature extraction model\n",
    "NUMBER_IN_FEATURES = 8\n",
    "train_x, train_y, test_x, test_y, anomaly_x, anomaly_y = create_train_test_split(extracted_data)\n",
    "hh102_extracted_model, hh102_extracted_history = get_final_model(train_x, train_y, test_x, test_y, True)\n",
    "# Get final feature extraction model\n",
    "\n",
    "# Inject anomalies\n",
    "random_anomaly, activate_anomaly = generate_anomalous_data(get_stats(np.row_stack((test_y, train_y, anomaly_y))), anomaly_x, anomaly_y, hh101_reversed_mapping)\n",
    "# Inject anomalies\n",
    "\n",
    "# Detect anomalies\n",
    "detect_anomalies(hh102_extracted_model, \"FE\", get_stats(np.row_stack((test_y, train_y, anomaly_y))), hh102_reversed_mapping, anomaly_x, anomaly_y)\n",
    "# Detect anomalies\n",
    "\n",
    "\n",
    "# hh102_extracted_error, hh102_extracted_r2s, hh102_extracted_predictions, hh102_extracted_sensor_errors = predict(anomaly_x, hh102_extracted_model, anomaly_y)\n",
    "\n",
    "# Plot mse graphs\n",
    "if (ENABLE_PLOTS):\n",
    "    plot_sensor_mse(hh102_selected_sensor_errors, hh102_extracted_sensor_errors)\n",
    "    plot_final_mse(hh101_selected_error, hh101_extracted_error, hh102_selected_error, hh102_extracted_error)\n",
    "    plot_final_training(hh101_selected_history, hh101_extracted_history, hh102_selected_history, hh102_extracted_history)\n",
    "# Plot mse graphs\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('projnew')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af2f049532c2b869fa3cc4e68ab09f7dac73e40ec50551a74fb31a492660dfa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
