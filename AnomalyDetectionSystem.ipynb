{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MSc Individual Project: Anomaly Detection for Assisted Independent Living\n",
    "\n",
    "Author: Christina Spanellis\n",
    " \n",
    "Sections 1-4 of this notebook define the necessary method definitions and constants for this project.\n",
    "\n",
    "Section 5 contains code to build and test the system\n",
    "\n",
    "## Sections\n",
    "### 1. [Data preparation and pre-processing](#section1)\n",
    "### 2. [Anomalous Data Generation Module](#section2)\n",
    "### 3. [Prediction Module](#section3)\n",
    "### 4. [Anomaly Detection Module](#section4)\n",
    "### 5. [Running the system](#section5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages for the project\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import pandas\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras import callbacks\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras import models\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "from keras.metrics import mean_squared_error\n",
    "from numpy import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## Section 1: Data preparation and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different data sets had to be cleaned and prepared to be in a suitable format for the prediction module. \n",
    "\n",
    "Data set 1: CASAS HH101\n",
    "\n",
    "Data set 2: CASAS HH102\n",
    "\n",
    "The below definitions define the constants and logic needed for loading and pre-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONSTANTS AND GLOBALS DEFINITION\n",
    "\n",
    "SENSOR_EVENT_WINDOW_SIZE = 19\n",
    "HH101_PREDICTION_SENSORS = [\"M003\", \"LS002\", \"M004\", \"M005\", \"LS005\", \"MA015\", \"M012\", \"M010\"]\n",
    "HH102_PREDICTION_SENSORS = [\"M007\", \"T105\", \"LS008\", \"M004\", \"M002\", \"LS010\", \"MA009\", \"M018\", \"LS002\"]\n",
    "NUM_FINAL_FEATURES = 10\n",
    "train_x = None\n",
    "NUMBER_IN_FEATURES = None\n",
    "DATASET = None\n",
    "NUMBER_PREDICTIONS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the hh101 data set\n",
    "def load_hh101():\n",
    "    # load the data set\n",
    "    hh101 = read_csv('datasets/hh101/hh101.csv', names=[\"Sensor\",1,2,\"Value\",\"Type\"])\n",
    "    hh101.drop(columns={1,2,\"Type\"}, inplace=True)\n",
    "    # replace string values and drop unwanted sensor readings\n",
    "    hh101 = hh101[hh101[\"Sensor\"].str.contains(\"BAT\") == False]\n",
    "    # hh101 = hh101[hh101[\"Sensor\"] != \"D001\"]\n",
    "    # hh101 = hh101[hh101[\"Sensor\"] != \"LS010\"]\n",
    "    hh101[\"Value\"] = hh101[\"Value\"].replace({\"ON\" : 1.0, \"OFF\" : 0.0})\n",
    "    hh101[\"Value\"] = hh101[\"Value\"].replace({\"ABSENT\" : 1.0, \"PRESENT\" : 0.0})\n",
    "    hh101[\"Value\"] = hh101[\"Value\"].replace({\"OPEN\" : 1.0, \"CLOSE\" : 0.0})\n",
    "    # creating a mapping of the sensor names to keep track of them\n",
    "    count = 0\n",
    "    hh101_sensor_id_mapping = {}\n",
    "    for sensor in hh101[\"Sensor\"].values:\n",
    "        if sensor not in hh101_sensor_id_mapping:\n",
    "            hh101_sensor_id_mapping[sensor] = count\n",
    "            count+=1\n",
    "    hh101_reversed_mapping = {y: x for x, y in hh101_sensor_id_mapping.items()}\n",
    "    return (hh101, hh101_sensor_id_mapping, hh101_reversed_mapping)\n",
    "\n",
    "# load the hh102 dataset \n",
    "def load_hh102():\n",
    "    # load the data set\n",
    "    hh102 = read_csv('datasets/hh102/hh102.csv', names=[\"Sensor\",1,2,\"Value\",\"Type\"])\n",
    "    hh102.drop(columns={1,2,\"Type\"}, inplace=True)\n",
    "    # replace string values and drop unwanted sensor readings\n",
    "    hh102 = hh102[hh102[\"Sensor\"].str.contains(\"BAT\") == False]\n",
    "    # hh102 = hh102[hh102[\"Sensor\"] != \"D001\"]\n",
    "    # hh102 = hh102[hh102[\"Sensor\"] != \"LS010\"]\n",
    "    hh102[\"Value\"] = hh102[\"Value\"].replace({\"ON\" : 1.0, \"OFF\" : 0.0})\n",
    "    hh102[\"Value\"] = hh102[\"Value\"].replace({\"ABSENT\" : 1.0, \"PRESENT\" : 0.0})\n",
    "    hh102[\"Value\"] = hh102[\"Value\"].replace({\"OPEN\" : 1.0, \"CLOSE\" : 0.0})\n",
    "    # creating a mapping of the sensor names to keep track of them\n",
    "    count = 0\n",
    "    hh102_sensor_id_mapping = {}\n",
    "    for sensor in hh102[\"Sensor\"].values:\n",
    "        if sensor not in hh102_sensor_id_mapping:\n",
    "            hh102_sensor_id_mapping[sensor] = count\n",
    "            count+=1\n",
    "    hh102_reversed_mapping = {y: x for x, y in hh102_sensor_id_mapping.items()}\n",
    "    return (hh102, hh102_sensor_id_mapping, hh102_reversed_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_sensors(dataset, sensors, mapping):\n",
    "    for i in range (len(sensors)):\n",
    "        sensors[i] = mapping[sensors[i]]\n",
    "    return dataset.drop(columns=sensors)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def pca_decomp(dataset, reversed_mapping, name):\n",
    "    dataset = dataset.copy()\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    dataset[dataset.columns] = min_max_scaler.fit_transform(dataset)\n",
    "    pca = PCA(n_components=20)\n",
    "    components = pca.fit_transform(dataset)\n",
    "    n_pcs= pca.n_components_ # get number of component\n",
    "    # get the index of the most important feature on EACH component\n",
    "    most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "    initial_feature_names = dataset.columns\n",
    "    # get the most important feature names\n",
    "    most_important_features = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "    most_important_features = [i for n, i in enumerate(most_important_features) if i not in most_important_features[:n]] \n",
    "    for i in range(NUM_FINAL_FEATURES):\n",
    "        most_important_features[i] = reversed_mapping[most_important_features[i]]\n",
    "    features = range(pca.n_components_)\n",
    "    _ = plt.figure(figsize=(15, 5))\n",
    "    _ = plt.bar(features, pca.explained_variance_)\n",
    "    _ = plt.xlabel('PCA feature')\n",
    "    _ = plt.ylabel('Variance')\n",
    "    _ = plt.xticks(features)\n",
    "    _ = plt.title(\"Importance of the Principal Components based on inertia\")\n",
    "    plt.savefig(\"data_plots/\"+name +\"/pca\")\n",
    "    plt.close()\n",
    "    most_important_features = most_important_features[:NUM_FINAL_FEATURES]\n",
    "    return most_important_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_stationary_and_autocorrelated():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method transforms the data set into a format where the columns represent the various sensor values and segment the data into 20 event sensor windows.\n",
    "\n",
    "This means that each row in the data set represents the activations for the previous 20 sensor event activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(dataset, sensor_id_mapping):\n",
    "    data = []\n",
    "    starting_date_time = datetime.strptime(dataset.index[0], '%Y-%m-%d %H:%M:%S.%f')\n",
    "    starting_date_time = starting_date_time.replace(microsecond=0)\n",
    "    sensor_vals = [0.0] * len(sensor_id_mapping)\n",
    "    event_count = 0 ## counter used to segment the data into sensor event windows\n",
    "    for i, row in dataset.iterrows():\n",
    "        curr_date_time =  datetime.strptime(i, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        curr_date_time = curr_date_time.replace(microsecond=0)\n",
    "        if (event_count >= SENSOR_EVENT_WINDOW_SIZE):\n",
    "            values = [starting_date_time.strftime(\"%m-%d-%Y %H:%M:%S\")]\n",
    "            values.extend(sensor_vals)\n",
    "            data.append(values)\n",
    "            starting_date_time = curr_date_time\n",
    "            sensor_vals = [0.0] * len(sensor_id_mapping)\n",
    "            event_count = 0\n",
    "        event_count +=1\n",
    "        if \"D\" in row[\"Sensor\"] or \"M\" in row[\"Sensor\"]:\n",
    "            if sensor_vals[sensor_id_mapping[row[\"Sensor\"]]] == 0:\n",
    "                sensor_vals[sensor_id_mapping[row[\"Sensor\"]]] = 1\n",
    "            else:\n",
    "                sensor_vals[sensor_id_mapping[row[\"Sensor\"]]] += 1\n",
    "        else:\n",
    "                sensor_vals[sensor_id_mapping[row[\"Sensor\"]]] = row[\"Value\"]\n",
    "\n",
    "    columns = [i for i in range (0,len(sensor_id_mapping))]\n",
    "    final_columns = [\"Time\"]\n",
    "    final_columns.extend(columns)\n",
    "    # set the index of the dataframe to be the time column\n",
    "    new_data = pandas.DataFrame.from_records(data, columns=final_columns)\n",
    "    new_data[\"Time\"] = pandas.to_datetime(new_data[\"Time\"], format='%m-%d-%Y %H:%M:%S')\n",
    "    new_data = new_data.set_index(\"Time\")\n",
    "    return new_data   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method plots all the data (non-normalised) after formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cleaned_data(data, reversed_mapping, name, path):\n",
    "    changed_legend = data.rename(columns = reversed_mapping)\n",
    "    ax = changed_legend.plot(subplots=True,figsize=(12,24), sharey=True, ylabel=\"Sensor Value\")\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.savefig(\"data_plots/\" + name + path, dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method transforms the series into a format suitable for a supervised learning problem.\n",
    "\n",
    "Eg. [Sensor1(t-1), Sensor2(t-1), Sensor1(t), Sensor2(t)]\n",
    "\n",
    "Where readings at t-1 represent the sensor activations for the period before the activations at t.\n",
    "\n",
    "Sensor activations at time t then become the ground truth values for activations at t-1 in the prediction module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_series_to_supervised(new_data, prediction_sensors, important_features, reversed_mapping):\n",
    "    df = new_data.copy()\n",
    "    # print(df)\n",
    "    # print(df)\n",
    "    # scale values\n",
    "    values = df.values\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    scaled_values = min_max_scaler.fit_transform(values)\n",
    "    normalized_df = pandas.DataFrame(scaled_values)\n",
    "    n_vars = len(normalized_df.columns)\n",
    "    time_Series = normalized_df.copy().set_index(df.index)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(1, 0, -1):\n",
    "        sequence = normalized_df.shift(i)\n",
    "        sequence = sequence.rename(columns=reversed_mapping)\n",
    "        sequence = sequence[important_features]\n",
    "        cols.append(sequence)\n",
    "        names += [('%s(t-%d)' % (label, i)) for label in sequence.columns]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, 1):\n",
    "        sequence = normalized_df.shift(-i)\n",
    "        sequence = sequence.rename(columns=reversed_mapping)\n",
    "        sequence = sequence[prediction_sensors]\n",
    "        cols.append(sequence.shift(-i))\n",
    "        names += [('%s(t)' % (label)) for label in sequence.columns]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    agg.dropna(inplace=True)\n",
    "    return (agg, normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method plots the activations for the sensors in the data set, grouped into figures by sensor type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normalised_sensor_activations(dataset_name, normalized_df, reversed_mapping):\n",
    "    changed_legend = normalized_df.rename(columns = reversed_mapping)\n",
    "    doors =[]\n",
    "    lights = []\n",
    "    temp = []\n",
    "    motion = []\n",
    "    for key in reversed_mapping:\n",
    "        if \"D\" in reversed_mapping[key]:\n",
    "            doors.append(key)\n",
    "        elif \"L\" in reversed_mapping[key]:\n",
    "            lights.append(key)\n",
    "        elif \"T1\" in reversed_mapping[key]:\n",
    "            temp.append(key)\n",
    "        else:\n",
    "            motion.append(key)\n",
    "    sensors = [doors, lights, temp, motion]\n",
    "    names = [\"Door Sensors\", \"Light Sensors\", \"Temperature Sensors\", \"Motion Sensors\"]\n",
    "    for i, sensor in enumerate(sensors):\n",
    "        if \"Door\" or \"Temperature\" in names[i]:\n",
    "            figsize = (11,4)\n",
    "        if ((len(sensor) / 2) % 2 == 0):\n",
    "            fig, axs = pyplot.subplots(int(len(sensor) / 2),2,  sharex=True, sharey=True, figsize=(11,8))\n",
    "        else:\n",
    "            fig, axs = pyplot.subplots(int(len(sensor) / 2)+1,2,  sharex=True, sharey=True, figsize=(11,4))\n",
    "\n",
    "        count = 0\n",
    "        for ax in axs.flat:\n",
    "            if (count < len(sensor)):\n",
    "                ax.plot(changed_legend[reversed_mapping[sensor[count]]])\n",
    "                ax.set_title(reversed_mapping[sensor[count]])\n",
    "                ax.set_xticks([0, 10000, 20000, 30000, 40000, 50000])\n",
    "                ax.set_yticks([0.0, 0.5, 1.0])\n",
    "                count += 1\n",
    "            else:\n",
    "                fig.delaxes(ax)\n",
    "        for ax in axs.flat:\n",
    "            ax.label_outer()\n",
    "        fig.suptitle(names[i])\n",
    "        fig.supxlabel(\"SEW\")\n",
    "        fig.supylabel(\"Sensor Value\")\n",
    "        pyplot.tight_layout()\n",
    "        pyplot.savefig(\"data_plots/\" + dataset_name + \"/\" +names[i],dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discover the features with the highest correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_correlations(dataset, path, name, reversed_mapping):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    scaled_values = min_max_scaler.fit_transform(dataset.values)\n",
    "    normalized_df = pandas.DataFrame(scaled_values)\n",
    "    normalized_df = normalized_df.rename(columns = reversed_mapping)\n",
    "    corrmat = normalized_df.corr(method='pearson', min_periods=100)\n",
    "    corrmat = np.abs(corrmat)\n",
    "    sns.set(context=\"paper\", font=\"monospace\")\n",
    "    f, ax = plt.subplots(figsize=(12, 9))\n",
    "    sns.heatmap(corrmat, vmax=0.8, square=True, xticklabels = True, yticklabels = True)\n",
    "    pyplot.title(name + \" Pearson correlation values between sensors (absolute valued).\")\n",
    "    pyplot.xlabel(\"Sensor ID\")\n",
    "    pyplot.ylabel(\"Sensor ID\")\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.savefig(\"data_plots/\" + path + name)\n",
    "    triangluar_corrmat = np.triu(corrmat, k=1)\n",
    "    values = np.where(triangluar_corrmat >= 0.6)\n",
    "    values = list(zip(values[0], values[1]))\n",
    "    for x in range (len(values)):\n",
    "        values[x] = (reversed_mapping[values[x][0]], reversed_mapping[values[x][1]], triangluar_corrmat[values[x][0]][values[x][1]])\n",
    "    return values \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to be separated out into training (70%), testing (20%) and anomalous portions (10%).\n",
    "\n",
    "The anomalous portion of the data is held back for synthetic anomaly injection to later be used to test the AD system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(dataset):\n",
    "    values = dataset.values\n",
    "    train_split = int(0.7 * len(values))\n",
    "    anomaly_split = int(0.9 * len(values))\n",
    "    train = values[:train_split, :]\n",
    "    test = values[train_split:anomaly_split, :]\n",
    "    anomalies = values[anomaly_split:, :]\n",
    "\n",
    "    train_x, train_y = train[:, :NUMBER_IN_FEATURES], train[:, NUMBER_IN_FEATURES:]\n",
    "\n",
    "    test_x, test_y = test[:, :NUMBER_IN_FEATURES], test[:, NUMBER_IN_FEATURES:]\n",
    "\n",
    "    anomaly_x, anomaly_y = anomalies[:, :NUMBER_IN_FEATURES], anomalies[:, NUMBER_IN_FEATURES:]\n",
    "\n",
    "    train_x = np.asarray(train_x).astype(np.float32)\n",
    "    train_y = np.asarray(train_y).astype(np.float32)\n",
    "    test_y = np.asarray(test_y).astype(np.float32)\n",
    "    test_x = np.asarray(test_x).astype(np.float32)\n",
    "    train_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\n",
    "    test_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))\n",
    "    return (train_x, train_y, test_x, test_y, anomaly_x, anomaly_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "\n",
    "## Section 3: Prediction Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section contains the logic to train models for the Prediction Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method to save trained models and plot their loss (used for experimentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model_and_plot(model, filename, history):\n",
    "    model.save(\"best_models/\" + filename +\"/best_model\", history)\n",
    "    pyplot.plot(history.history['loss'], label='Loss')\n",
    "    pyplot.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    pyplot.xlabel(\"Epochs\")\n",
    "    pyplot.ylabel(\"Loss\")\n",
    "    pyplot.title(\"Training and validation loss of final \"+ filename.upper() +\" model.\")\n",
    "    pyplot.tight_layout()\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig(\"best_models/\"+filename.upper()+\"/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a model suitable for hyper parameter tuning in in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hp.Int('input_lstm_layer', min_value = 50, max_value = 500, step = 50), input_shape=(train_x.shape[1], train_x.shape[2]), return_sequences = True))\n",
    "    for i in range (hp.Int('n_layers', 1, 3)):\n",
    "            model.add(LSTM(hp.Int(f'hidden_lstm_layer_{i}', min_value = 50, max_value = 500, step = 50), return_sequences = True))\n",
    "    model.add(LSTM(hp.Int(f'final_lstm_layer', min_value = 50, max_value = 500, step = 50)))\n",
    "    model.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    model.add(Dense(NUMBER_PREDICTIONS, activation = hp.Choice('dense_activation', values=['relu', 'sigmoid'],default='relu')))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics = ['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and use the keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(x_train, y_train, name):\n",
    "\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        build_model,\n",
    "        objective='mse',\n",
    "        max_trials=20,\n",
    "        directory=\"tensorflow/\"+name+\"/\",\n",
    "        project_name=\"models\",\n",
    "        overwrite = False\n",
    "    )\n",
    "    tuner.search(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size = 128,\n",
    "        validation_split=0.2,\n",
    "        epochs = 500,\n",
    "        callbacks=[callbacks.TensorBoard(log_dir=\"/tmp/tb_logs/\"+name, histogram_freq=1), callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30), callbacks.ModelCheckpoint('best_model_es.h5',monitor='val_loss',mode='min',save_best_only=True)],\n",
    "\n",
    "    )\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train new model with best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def get_final_model(train_x, train_y, test_x, test_y, tuner):\n",
    "    model = None\n",
    "    filename = \"best_models/\" + DATASET + \"/best_model\"\n",
    "    if os.path.isdir(filename):\n",
    "        model = models.load_model(filename)\n",
    "    else:\n",
    "        model = train_final_model(train_x, train_y, test_x, test_y, tuner)\n",
    "    return model\n",
    "\n",
    "def train_final_model(train_x, train_y, test_x, test_y, tuner):\n",
    "    model = tuner.hypermodel.build(tuner.get_best_hyperparameters()[0])\n",
    "    history = model.fit(train_x, train_y, epochs=1000, validation_data=(test_x, test_y))\n",
    "    save_model_and_plot(model, DATASET, history)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import concatenate\n",
    "# from math import sqrt\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# # make a prediction\n",
    "# yhat = model.predict(test_x)\n",
    "# test_x = test_x.reshape((test_x.shape[0], features))\n",
    "# print(test_x)\n",
    "# print(yhat)\n",
    "# # # calculate RMSE\n",
    "# rmse = sqrt(mean_squared_error(test_y, yhat))\n",
    "# print('Test RMSE: %.3f' % rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Section 2: Anomamlous Data Generation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section contains the logic for the Anomalous Data Generation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method takes the test data and inserts anomalies into a specified fraction of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try generating some anomalous data\n",
    "# So decide on the number of anomalous samples.. for example 10\n",
    "# Then just affect one 20 event sensor window\n",
    "# One idea is to just scale data to be bigger than 1, in theory this is an anomaly but isn't really in the expected range of input...\n",
    "# https://github.com/tirthajyoti/Synthetic-data-gen/blob/master/Notebooks/Time%20series%20synthesis%20with%20anomaly.ipynb\n",
    "\n",
    " # retrieves the mean, min and max of the different features in the data set\n",
    "def get_stats(dataset):\n",
    "    return (dataset.mean(axis=0), dataset.min(axis=0), dataset.max(axis=0))\n",
    "\n",
    "def generate_anomalous_data(stats, anomaly_x, anomaly_y):\n",
    "    anomaly_split = 100\n",
    "    means = stats[0]\n",
    "    mins = stats[1]\n",
    "    maxs = stats[2]\n",
    "    increase_mag = True\n",
    "    number_anomalies = 10\n",
    "    # now need to randomly select one row of data to to alter, so as to not alter the underlying sequence\n",
    "    anomaly_scale = 1.1\n",
    "    # types of generated anomalies?\n",
    "    # - random\n",
    "    # - intentional anomalies\n",
    "    #   - swap anomalies\n",
    "    #   - activate/deactivate anomalies\n",
    "    # 1. random anomalies\n",
    "    random_anomaly_x = anomaly_x[:anomaly_split, :]\n",
    "    generate_random_anomaly(anomaly_y[:anomaly_split, :], number_anomalies, anomaly_scale, maxs, mins)\n",
    "\n",
    "    # 2. intentional anomalies\n",
    "    # # 2.1. one simple anomaly is all sensor values = 0\n",
    "    zero_anomaly_x = anomaly_x[:anomaly_split*2, :]\n",
    "    generate_intentional_anomaly(anomaly_y[anomaly_split:anomaly_split*2, :], number_anomalies, anomaly_scale, maxs, mins, \"zero\")\n",
    "\n",
    "    # # 2.2 activate some portion of non-active sensors\n",
    "    activate_anomaly_x = anomaly_x[:anomaly_split*3, :]\n",
    "    generate_intentional_anomaly(anomaly_y[anomaly_split*2:anomaly_split*3, :], number_anomalies, anomaly_scale, maxs, mins, \"activate\")\n",
    "\n",
    "    deactivate_anomaly_x = anomaly_x[:anomaly_split*4, :]\n",
    "    # # 2.3 de-activate active, activate portion of non-active\n",
    "    generate_intentional_anomaly(anomaly_y[anomaly_split*3:anomaly_split*4, :], number_anomalies, anomaly_scale, maxs, mins, \"deactivate\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_anomaly(anomaly_y, number_anomalies, anomaly_scale, maxs, mins):\n",
    "    row_ids = np.random.choice(anomaly_y.shape[0], size=number_anomalies, replace=False)\n",
    "    random_anomalous_y = pandas.DataFrame(anomaly_y)\n",
    "    random_anomalous_y_copy = random_anomalous_y.copy()\n",
    "    for row_id in row_ids:\n",
    "        new_data = [0.0] * len(random_anomalous_y.columns)\n",
    "        for y in range(len(new_data)):\n",
    "            # if increase_mag:\n",
    "                new_data[y] = np.random.uniform(low=mins[y], high=anomaly_scale * (maxs[y] - mins[y]))\n",
    "            # else:\n",
    "                # random_anomalous_y[row_id][y] = np.random.uniform(low=anomaly_scale * (maxs[y] - mins[y]), high=anomaly_scale * (maxs[y] - mins[y]))\n",
    "        random_anomalous_y.loc[row_id] = new_data\n",
    "    plot_anomalsed_data(row_ids, random_anomalous_y, random_anomalous_y_copy, \"random_anomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_intentional_anomaly(anomaly_y, number_anomalies, anomaly_scale, maxs, mins, type):\n",
    "    row_ids = np.random.choice(anomaly_y.shape[0], size=number_anomalies, replace=False)\n",
    "    random_anomalous_y = pandas.DataFrame(anomaly_y)\n",
    "    random_anomalous_y_copy = random_anomalous_y.copy()\n",
    "    for row_id in row_ids:\n",
    "        new_data = [0.0] * len(random_anomalous_y.columns)\n",
    "        for y in range(len(new_data)):\n",
    "            if type==\"deactivate\":\n",
    "                if (random_anomalous_y.iloc[row_id][y] != 0.0):\n",
    "                    new_data[y] = np.random.uniform(low=mins[y], high=(maxs[y] - mins[y]))\n",
    "            elif type==\"activate\":\n",
    "                if (random_anomalous_y.iloc[row_id][y] == 0.0):\n",
    "                    new_data[y] = np.random.uniform(low=mins[y], high= (maxs[y] - mins[y]))\n",
    "        random_anomalous_y.loc[row_id] = new_data\n",
    "    plot_anomalsed_data(row_ids, random_anomalous_y, random_anomalous_y_copy, \"intentional_anomaly\" + type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalsed_data(row_ids, dataset, dataset_copy, type):\n",
    "    colors = [\"cornflowerblue\",\"lightsteelblue\",\"mediumblue\",\"blue\",\"slateblue\",\"navy\",\"royalblue\", \"dodgerblue\",\"red\"]\n",
    "    fig, (ax1, ax2) = pyplot.subplots(1,2, figsize=(17,8), dpi=300, sharey=True)\n",
    "    ax1.set_xlabel('SEW')\n",
    "    ax1.set_ylabel('Sensor Value')\n",
    "    ax2.set_xlabel('SEW')\n",
    "    ax2.set_ylabel('Sensor Value')\n",
    "    fig.suptitle(\"Normal data vs data with random synthetic anomaly injection\")\n",
    "    dataset_copy.plot(color=colors[:-1], ax=ax1)\n",
    "    anomaly = [0.0] * len(dataset.index)\n",
    "    for row in row_ids:\n",
    "        anomaly[row] = dataset.iloc[row].max()\n",
    "        dataset.loc[row] = [0.0]* NUMBER_PREDICTIONS\n",
    "    dataset[\"Anomaly\"] = anomaly\n",
    "    dataset.plot(color=colors, ax=ax2)\n",
    "    plt.legend(loc = \"upper left\")\n",
    "    plt.tight_layout()\n",
    "    pyplot.savefig(\"data_plots/\" + DATASET + \"/\" + type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## Section 4: Anomaly Detection Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method estimates the likelihood of an anomaly occuring by comparing the predicted value to the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomaly(anomaly_x, anomaly_y, predictions):\n",
    "    anomaly_free_x_values = anomaly_x\n",
    "    anomaly_inserted_y_vales = anomaly_y\n",
    "    \n",
    "    for i, prediction in enumerate(predictions):\n",
    "        anomaly_probability = 0.0\n",
    "        for x, value in enumerate(prediction):\n",
    "            anomaly_probability += abs(value - anomaly_inserted_y_vales[i][x])\n",
    "        print(\"Probability of anomaly: \", anomaly_probability / len(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## Section 5: Running the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the code to run the system on data sets 1 and 2 (note these runs use the final model found for the Prediction Module during hyper parameter tuning)\n",
    "\n",
    "1. [HH101](#hh101)\n",
    "2. [HH102](#hh102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hh101'></a>\n",
    "\n",
    "### HH101 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET= \"hh101\"\n",
    "# # Data pre-processing \n",
    "# hh101, hh101_sensor_id_mapping, hh101_reversed_mapping = load_hh101()\n",
    "# hh101_drop = [\"D002\", \"MA016\", \"LS010\", \"LS009\", \"LS013\", \"LS006\", \"M006\", \"MA013\", \"LS015\",\"T101\", \"T102\", \"T103\", \"T104\", \"T105\"]\n",
    "# hh101 = transform_data(hh101, hh101_sensor_id_mapping)\n",
    "# # Data pre-processing\n",
    "\n",
    "# # Feature Selection\n",
    "# hh101_removed = remove_sensors(hh101, hh101_drop, hh101_sensor_id_mapping)\n",
    "# most_important_features = pca_decomp(hh101_removed, hh101_reversed_mapping,\"hh101\")\n",
    "# # Feature Selection\n",
    "\n",
    "# # Set up data for prediction module\n",
    "# hh101_data, plot_data = transform_series_to_supervised(hh101, HH101_PREDICTION_SENSORS, most_important_features, hh101_reversed_mapping)\n",
    "# NUMBER_IN_FEATURES = len(most_important_features)\n",
    "# NUMBER_PREDICTIONS = len(HH101_PREDICTION_SENSORS)\n",
    "# train_x, train_y, test_x, test_y, anomaly_x, anomaly_y = create_train_test_split(hh101_data)\n",
    "# # Set up data for prediction module\n",
    "\n",
    "# # Find best hyper-params and train final model, or load best model from file\n",
    "# tuner = tune_model(train_x, train_y, \"hh101\")\n",
    "# final_model = get_final_model(train_x, train_y, test_x, test_y, tuner)\n",
    "# # Find best hyper-params and train final model, or load best model from file\n",
    "\n",
    "# numbers = np.row_stack((test_y, train_y, anomaly_y))\n",
    "# generate_anomalous_data(get_stats(numbers), anomaly_x, anomaly_y)\n",
    "\n",
    "# plot_normalised_sensor_activations(\"hh101\", plot_data, hh101_reversed_mapping)\n",
    "plot_cleaned_data(hh101, hh101_reversed_mapping, \"hh101\", \"/cleaned_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %load_ext tensorboard\n",
    "\n",
    "#   Load best model and make predictions\n",
    "# model = load_shit()\n",
    "# predictions = predict_shit()\n",
    "# Load model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /Users/christinaspanellis/Desktop/MAC/AAL_ICL/tensorflow/hh102/logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_cleaned_data(hh101, hh101_reversed_mapping, \"hh101\", \"/cleaned_data.png\")\n",
    "# plot_normalised_sensor_activations(\"hh101\", plot_data, hh101_reversed_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hh102'></a>\n",
    "\n",
    "### HH102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET = \"hh102\"\n",
    "# # Data pre-processing \n",
    "# hh102, hh102_sensor_id_mapping, hh102_reversed_mapping = load_hh102()\n",
    "# hh102_drop = [\"LS013\", \"LS006\", \"LS011\", \"M011\", \"MA010\", \"LS012\", \"LS015\", \"LS009\", \"LS023\", \"T101\", \"T102\", \"T103\", \"T104\"]\n",
    "# hh102 = transform_data(hh102, hh102_sensor_id_mapping)\n",
    "# # Data pre-processing \n",
    "\n",
    "# # Feature Selection\n",
    "# hh102_removed = remove_sensors(hh102, hh102_drop, hh102_sensor_id_mapping)\n",
    "# most_important_features = pca_decomp(hh102_removed, hh102_reversed_mapping, \"hh102\")\n",
    "# # Feature Selection\n",
    "\n",
    "# # Set up data for prediction module\n",
    "# hh102_data, plot_data = transform_series_to_supervised(hh102, HH102_PREDICTION_SENSORS, most_important_features, hh102_reversed_mapping)\n",
    "# NUMBER_IN_FEATURES = len(most_important_features)\n",
    "# NUMBER_PREDICTIONS = len(HH102_PREDICTION_SENSORS)\n",
    "# train_x, train_y, test_x, test_y, anomaly_x, anomaly_y = create_train_test_split(hh102_data)\n",
    "# # Set up data for prediction module\n",
    "\n",
    "# # This was used for finding the best model, uncomment to see tuner in action\n",
    "# # model = tune_model(train_x, train_y, \"hh102\")\n",
    "# # This was used for finding the best model, uncomment to see tuner in action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this cell will create various plots of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_cleaned_data(hh102, hh102_reversed_mapping, \"hh102\" , \"/cleaned_data.png\")\n",
    "# plot_normalised_sensor_activations(\"hh102\", plot_data, hh102_reversed_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tune_model(train_x, train_y, \"hh102\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mscproj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d17618d7e163c2a95ac78e25d7966a2827f51b90d43fb3fa6b15fe17847d1ec4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
